#!/usr/bin/env python3
"""
TRUE PARALLEL LLM INFERENCE TESTER
==================================

This script implements REAL parallel processing for LLM inference testing.
Fixes the sequential processing issue in the original script.

üîß KEY FIXES:
- Batch processing: Process all queries in a single batch
- True GPU parallelization using tensor batching
- Proper attention masking for variable-length inputs
- Efficient memory management
- Real concurrent execution measurement

TARGET: Complete 10 concurrent queries in actual parallel execution
"""

import os
import sys
import time
import json
import statistics
import traceback
import psutil
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# Import dependencies
try:
    import torch
    import numpy as np
    import faiss
    import pickle
    print(f"‚úÖ PyTorch: {torch.__version__}")
    if torch.cuda.is_available():
        print(f"üöÄ CUDA: {torch.cuda.get_device_name(0)}")
        print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}GB")
    else:
        print("‚ö†Ô∏è CUDA: Not available")
except ImportError as e:
    print(f"‚ùå Required dependencies missing: {e}")
    sys.exit(1)

class TrueParallelBookChatBot:
    """True parallel processing LLM with batch inference"""
    
    def __init__(self):
        print("üöÄ Initializing TrueParallelBookChatBot...")
        self.setup_imports()
        self.load_optimized_components()
        
    def setup_imports(self):
        """Import required modules with error handling"""
        try:
            sys.path.append(os.path.dirname(os.path.abspath(__file__)))
            
            # Import from transformers
            global AutoTokenizer, AutoModelForCausalLM
            from transformers import AutoTokenizer, AutoModelForCausalLM
            
            # Import PEFT
            global PeftModel
            from peft import PeftModel
            
            print("‚úÖ All required modules imported successfully")
            
        except ImportError as e:
            print(f"‚ùå Failed to import required modules: {e}")
            raise
    
    def load_optimized_components(self):
        """Load all components optimized for true parallel processing"""
        
        # Configuration
        self.BASE_MODEL_PATH = "/nlsasfs/home/ledgerptf/ashsa/models/sarvam1"
        self.FINETUNED_MODEL_PATH = "fine_tuned_sarvam_books"
        self.INDEX_FILE = "faiss_index.ivf"
        self.META_FILE = "chunks_meta.jsonl"
        self.VECTORIZER_FILE = "tfidf_vectorizer.pkl"
        self.SVD_FILE = "svd_model.pkl"
        self.CHUNKS_FILE = "data_books/chunks.jsonl"
        
        # Optimized parameters for batch processing
        self.MAX_NEW_TOKENS = 128
        self.TEMPERATURE = 0.7
        self.TOP_P = 0.9
        self.TOP_K_RETRIEVAL = 3
        self.MAX_CONTEXT_LENGTH = 1024
        self.BATCH_SIZE = 10  # Process up to 10 queries in one batch
        
        # Thread-safe FAISS lock (only for retrieval)
        self.faiss_lock = threading.RLock()
        
        print("üìö Loading models for TRUE parallel processing...")
        self._load_tokenizer()
        self._load_model()
        self._load_retrieval_system()
        
        print("‚úÖ TrueParallelBookChatBot ready for REAL parallel inference!")
        
    def _load_tokenizer(self):
        """Load tokenizer with padding support"""
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.BASE_MODEL_PATH, 
                local_files_only=True, 
                use_fast=True
            )
            print("‚úÖ Loaded tokenizer via AutoTokenizer.")
        except Exception as e:
            print("‚ö†Ô∏è AutoTokenizer failed, using PreTrainedTokenizerFast fallback...")
            from transformers import PreTrainedTokenizerFast
            self.tokenizer = PreTrainedTokenizerFast(
                tokenizer_file=os.path.join(self.BASE_MODEL_PATH, "tokenizer.json"),
                use_fast=True,
            )
            print("‚úÖ Loaded tokenizer via PreTrainedTokenizerFast.")
        
        # Configure pad token for batch processing
        if self.tokenizer.pad_token_id is None:
            if hasattr(self.tokenizer, 'eos_token_id') and self.tokenizer.eos_token_id is not None:
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            else:
                vocab_size = getattr(self.tokenizer, 'vocab_size', len(self.tokenizer))
                self.tokenizer.pad_token_id = vocab_size - 1
        
        # CRITICAL: Set padding side to LEFT for decoder-only models
        self.tokenizer.padding_side = 'left'
        
        print("‚úÖ Tokenizer configured for batch processing with LEFT padding")
        
    def _load_model(self):
        """Load model optimized for batch inference"""
        print("üìÇ Loading base model for batch processing...")
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.BASE_MODEL_PATH,
            local_files_only=True,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True,
            use_cache=True,
            low_cpu_mem_usage=True,
        )
        
        # Load fine-tuned adapter
        if os.path.exists(self.FINETUNED_MODEL_PATH):
            print("üéØ Loading fine-tuned adapter...")
            self.model = PeftModel.from_pretrained(self.model, self.FINETUNED_MODEL_PATH)
        
        self.model.eval()
        
        # Get device
        self.device = next(self.model.parameters()).device
        
        # Optimize for batch processing
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            try:
                torch.backends.cuda.enable_flash_sdp(True)
                print("‚úÖ Flash attention enabled for batch processing")
            except:
                pass
        
        print(f"‚úÖ Model ready for TRUE PARALLEL BATCH PROCESSING on {self.device}")
        
    def _load_retrieval_system(self):
        """Load FAISS retrieval system"""
        print("üîç Loading FAISS retrieval system...")
        
        # Load FAISS index
        self.faiss_index = faiss.read_index(self.INDEX_FILE)
        
        # Load metadata
        self.metadata = []
        with open(self.META_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                self.metadata.append(json.loads(line))
        
        # Load chunks
        self.chunks_with_text = {}
        with open(self.CHUNKS_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                chunk = json.loads(line)
                self.chunks_with_text[chunk["id"]] = chunk["text"]
        
        # Load vectorizer and SVD
        with open(self.VECTORIZER_FILE, 'rb') as f:
            self.vectorizer = pickle.load(f)
        with open(self.SVD_FILE, 'rb') as f:
            self.svd = pickle.load(f)
        
        print(f"‚úÖ Retrieval system loaded with {self.faiss_index.ntotal} vectors")
        
    def retrieve_for_query(self, query):
        """Thread-safe retrieval for a single query"""
        with self.faiss_lock:
            # Transform query to vector
            query_tfidf = self.vectorizer.transform([query])
            query_vec = self.svd.transform(query_tfidf).astype(np.float32)
            faiss.normalize_L2(query_vec)
            
            # Search for relevant chunks
            scores, indices = self.faiss_index.search(query_vec, self.TOP_K_RETRIEVAL)
            
            # Collect relevant chunks
            relevant_chunks = []
            for i in range(self.TOP_K_RETRIEVAL):
                if indices[0][i] >= 0 and scores[0][i] > 0.1:
                    meta = self.metadata[indices[0][i]]
                    chunk_id = meta["id"]
                    text_content = self.chunks_with_text.get(chunk_id, "")
                    relevant_chunks.append({
                        "text": text_content,
                        "source": meta.get("source", "unknown"),
                        "page": meta.get("page", "unknown"),
                        "score": float(scores[0][i])
                    })
            
            return relevant_chunks
    
    def create_prompt(self, user_question, relevant_chunks):
        """Create optimized prompt"""
        if not relevant_chunks:
            return f"Question: {user_question}\nAnswer:"
        
        # Use top chunks for context
        context_parts = []
        for chunk in relevant_chunks[:2]:  # Use top 2 chunks
            context_parts.append(chunk["text"][:400])  # Limit chunk size
        
        context = "\n\n".join(context_parts)
        
        prompt = f"""Context: {context}

Question: {user_question}
Answer:"""
        
        return prompt
    
    def batch_inference(self, prompts):
        """TRUE PARALLEL BATCH INFERENCE - Process all prompts simultaneously"""
        print(f"üöÄ Starting BATCH INFERENCE for {len(prompts)} prompts")
        batch_start_time = time.time()
        
        try:
            # Tokenize all prompts in batch with padding
            print("üìù Tokenizing batch...")
            tokenize_start = time.time()
            
            inputs = self.tokenizer(
                prompts,
                return_tensors="pt",
                padding=True,  # Essential for batch processing
                truncation=True,
                max_length=self.MAX_CONTEXT_LENGTH,
                return_attention_mask=True  # Essential for proper attention
            ).to(self.device)
            
            tokenize_time = time.time() - tokenize_start
            print(f"‚úÖ Tokenization completed in {tokenize_time:.2f}s")
            print(f"üìä Batch shape: {inputs['input_ids'].shape}")
            
            # Generate for entire batch simultaneously
            print("‚ö° Starting PARALLEL GENERATION...")
            generation_start = time.time()
            
            with torch.no_grad():
                # CRITICAL: This processes ALL inputs in PARALLEL on GPU
                output_ids = self.model.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_new_tokens=self.MAX_NEW_TOKENS,
                    temperature=self.TEMPERATURE,
                    top_p=self.TOP_P,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    use_cache=True,
                )
            
            generation_time = time.time() - generation_start
            print(f"‚úÖ PARALLEL GENERATION completed in {generation_time:.2f}s")
            
            # Decode all responses
            print("üî§ Decoding responses...")
            decode_start = time.time()
            
            responses = []
            input_lengths = inputs['attention_mask'].sum(dim=1)  # Actual lengths per input
            
            for i in range(len(prompts)):
                # Extract only new tokens for this response
                input_length = input_lengths[i].item()
                new_tokens = output_ids[i][input_length:]
                
                response = self.tokenizer.decode(
                    new_tokens, 
                    skip_special_tokens=True, 
                    clean_up_tokenization_spaces=False
                )
                responses.append(response.strip())
            
            decode_time = time.time() - decode_start
            total_batch_time = time.time() - batch_start_time
            
            print(f"‚úÖ Decoding completed in {decode_time:.2f}s")
            print(f"üéØ TOTAL BATCH TIME: {total_batch_time:.2f}s")
            
            # Calculate metrics
            total_new_tokens = sum([len(output_ids[i][input_lengths[i]:]) for i in range(len(prompts))])
            avg_tokens_per_second = total_new_tokens / generation_time if generation_time > 0 else 0
            
            return {
                "responses": responses,
                "total_batch_time": total_batch_time,
                "tokenize_time": tokenize_time,
                "generation_time": generation_time,
                "decode_time": decode_time,
                "total_new_tokens": total_new_tokens,
                "avg_tokens_per_second": avg_tokens_per_second,
                "parallel_efficiency": len(prompts) / (generation_time / (generation_time / len(prompts))) if generation_time > 0 else 1.0
            }
            
        except Exception as e:
            print(f"‚ùå Batch inference failed: {e}")
            print(f"üîç Error details: {traceback.format_exc()}")
            raise
    
    def parallel_chat_batch(self, questions):
        """Process multiple questions in TRUE parallel fashion"""
        print(f"üöÄ Processing {len(questions)} questions in PARALLEL")
        batch_start_time = time.time()
        
        try:
            # Step 1: Retrieve context for all questions (can be parallelized)
            print("üîç Retrieving context for all questions...")
            retrieval_start = time.time()
            
            # Use ThreadPoolExecutor for concurrent retrieval
            all_contexts = []
            with ThreadPoolExecutor(max_workers=len(questions)) as executor:
                retrieval_futures = {
                    executor.submit(self.retrieve_for_query, question): i 
                    for i, question in enumerate(questions)
                }
                
                # Collect results in order
                context_results = [None] * len(questions)
                for future in as_completed(retrieval_futures):
                    index = retrieval_futures[future]
                    context_results[index] = future.result()
            
            retrieval_time = time.time() - retrieval_start
            print(f"‚úÖ Context retrieval completed in {retrieval_time:.2f}s")
            
            # Step 2: Create all prompts
            print("üìù Creating prompts...")
            prompts = []
            for i, question in enumerate(questions):
                prompt = self.create_prompt(question, context_results[i])
                prompts.append(prompt)
            
            # Step 3: TRUE PARALLEL BATCH INFERENCE
            print("‚ö° Starting TRUE PARALLEL INFERENCE...")
            batch_result = self.batch_inference(prompts)
            
            total_time = time.time() - batch_start_time
            
            # Combine results
            results = []
            for i, (question, response) in enumerate(zip(questions, batch_result["responses"])):
                results.append({
                    "question": question,
                    "response": response,
                    "chunks_found": len(context_results[i]) if context_results[i] else 0,
                    "response_length": len(response)
                })
            
            return {
                "results": results,
                "total_time": total_time,
                "retrieval_time": retrieval_time,
                "batch_inference_time": batch_result["total_batch_time"],
                "generation_time": batch_result["generation_time"],
                "total_new_tokens": batch_result["total_new_tokens"],
                "avg_tokens_per_second": batch_result["avg_tokens_per_second"],
                "queries_per_second": len(questions) / total_time,
                "true_parallel": True,
                "error": None
            }
            
        except Exception as e:
            total_time = time.time() - batch_start_time
            error_msg = str(e)
            print(f"‚ùå Error in parallel_chat_batch: {error_msg}")
            
            return {
                "results": [],
                "total_time": total_time,
                "error": error_msg,
                "true_parallel": False
            }

class TrueParallelTester:
    """Tester for true parallel LLM inference"""
    
    def __init__(self, num_queries=10):
        self.num_queries = num_queries
        self.bot = None
        
        # Test queries
        self.test_queries = [
            "What is remote sensing?",
            "Explain electromagnetic spectrum in detail.",
            "How do satellites work for earth observation?",
            "What are different remote sensing platforms?",
            "Explain active vs passive remote sensing.",
            "How does radar remote sensing work?",
            "What are atmospheric effects on remote sensing?",
            "Explain different types of resolution in remote sensing.",
            "How is remote sensing data processed?",
            "What are applications of remote sensing in agriculture?"
        ][:num_queries]
        
        print(f"üéØ TRUE PARALLEL TESTER INITIALIZED")
        print(f"üìä Testing {num_queries} queries in TRUE parallel execution")
        print("=" * 60)
    
    def setup_bot(self):
        """Setup the true parallel bot"""
        try:
            self.bot = TrueParallelBookChatBot()
            return True
        except Exception as e:
            print(f"‚ùå Failed to setup bot: {e}")
            print(f"üîç Error details: {traceback.format_exc()}")
            return False
    
    def verify_bot(self):
        """Verify bot with a single query"""
        try:
            print("üß™ Running bot verification...")
            
            test_query = ["What is remote sensing?"]
            result = self.bot.parallel_chat_batch(test_query)
            
            if result.get("error"):
                print(f"‚ùå Bot verification failed: {result['error']}")
                return False
                
            if not result.get("results") or len(result["results"]) == 0:
                print("‚ùå Bot verification failed: No results")
                return False
            
            response = result["results"][0]["response"]
            if not response or len(response) < 5:
                print(f"‚ùå Bot verification failed: Invalid response (length: {len(response)})")
                return False
            
            print(f"‚úÖ Bot verification passed!")
            print(f"   Response length: {len(response)} characters")
            print(f"   Generation time: {result.get('generation_time', 0):.2f}s")
            print(f"   Total time: {result.get('total_time', 0):.2f}s")
            print(f"   Tokens/sec: {result.get('avg_tokens_per_second', 0):.1f}")
            
            # Show preview
            preview = response[:100].replace('\n', ' ')
            print(f"   Preview: {preview}...")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Bot verification failed: {e}")
            return False
    
    def run_true_parallel_test(self):
        """Run the true parallel test"""
        print(f"\n‚ö° STARTING TRUE PARALLEL TEST")
        print(f"üéØ Processing {self.num_queries} queries in REAL parallel execution")
        print("=" * 60)
        
        test_start_time = time.time()
        
        # Execute TRUE PARALLEL batch processing
        result = self.bot.parallel_chat_batch(self.test_queries)
        
        test_end_time = time.time()
        total_test_time = test_end_time - test_start_time
        
        print(f"\n‚ö° TRUE PARALLEL TEST COMPLETED!")
        print(f"‚è±Ô∏è  Total Time: {total_test_time:.2f} seconds")
        print("=" * 60)
        
        return result, total_test_time
    
    def analyze_results(self, result, total_test_time):
        """Analyze true parallel test results"""
        print("\nüöÄ TRUE PARALLEL PERFORMANCE ANALYSIS")
        print("=" * 60)
        
        if result.get("error"):
            print(f"‚ùå Test failed with error: {result['error']}")
            return
        
        results = result.get("results", [])
        successful_queries = len(results)
        
        print(f"üìä EXECUTION SUMMARY")
        print(f"   ‚úÖ Successful: {successful_queries}/{self.num_queries}")
        print(f"   ‚è±Ô∏è  Total Time: {total_test_time:.2f}s")
        print(f"   üöÄ TRUE PARALLEL: {result.get('true_parallel', False)}")
        
        if not results:
            print("‚ùå No successful queries to analyze")
            return
        
        # Timing analysis
        retrieval_time = result.get("retrieval_time", 0)
        generation_time = result.get("generation_time", 0)
        batch_time = result.get("batch_inference_time", 0)
        
        print(f"\n‚ö° TRUE PARALLEL TIMING ANALYSIS")
        print(f"   üîç Retrieval Time: {retrieval_time:.2f}s")
        print(f"   ‚ö° Generation Time: {generation_time:.2f}s (ALL queries processed simultaneously)")
        print(f"   üì¶ Total Batch Time: {batch_time:.2f}s")
        print(f"   üéØ Total Test Time: {total_test_time:.2f}s")
        
        # Speed metrics
        tokens_per_sec = result.get("avg_tokens_per_second", 0)
        queries_per_sec = result.get("queries_per_second", 0)
        total_tokens = result.get("total_new_tokens", 0)
        
        print(f"\nüöÄ SPEED METRICS")
        print(f"   ‚ö° Generation Speed: {tokens_per_sec:.1f} tokens/sec")
        print(f"   üìà Query Throughput: {queries_per_sec:.2f} queries/sec")
        print(f"   üéØ Total Tokens Generated: {total_tokens}")
        
        # Parallel efficiency analysis
        single_query_estimate = generation_time  # In true parallel, this is the time for ALL queries
        theoretical_sequential_time = single_query_estimate * self.num_queries
        actual_time = total_test_time
        parallel_efficiency = theoretical_sequential_time / actual_time if actual_time > 0 else 0
        
        print(f"\nüîÑ PARALLELISM ANALYSIS")
        print(f"   üéØ TRUE PARALLEL EXECUTION: ‚úÖ")
        print(f"   ‚ö° All {self.num_queries} queries processed simultaneously in {generation_time:.2f}s")
        print(f"   üìä Theoretical Sequential Time: {theoretical_sequential_time:.1f}s")
        print(f"   üöÄ Actual Parallel Time: {actual_time:.1f}s")
        print(f"   ‚ö° Parallel Efficiency: {parallel_efficiency:.1f}x speedup")
        
        if parallel_efficiency > 5:
            print(f"   ‚úÖ EXCELLENT: True parallel processing achieved!")
        elif parallel_efficiency > 2:
            print(f"   ‚úÖ GOOD: Significant parallel speedup achieved!")
        else:
            print(f"   ‚ö†Ô∏è LIMITED: Parallel efficiency could be improved")
        
        # Performance target check
        target_time = 20  # seconds
        if total_test_time <= target_time:
            print(f"\nüéØ TARGET ACHIEVED: Completed in {total_test_time:.1f}s (target: ‚â§{target_time}s)")
            print(f"   üöÄ SUCCESS: True parallel processing working!")
        else:
            improvement_needed = total_test_time - target_time
            print(f"\n‚ö° TARGET MISSED: {total_test_time:.1f}s (need {improvement_needed:.1f}s improvement)")
        
        # Quality analysis
        print(f"\nüìù QUALITY ANALYSIS")
        response_lengths = [r["response_length"] for r in results]
        avg_length = statistics.mean(response_lengths) if response_lengths else 0
        print(f"   üìä Average Response Length: {avg_length:.0f} characters")
        print(f"   üìà Response Range: {min(response_lengths)}-{max(response_lengths)} chars")
        
        # Sample responses  
        for i, result_item in enumerate(results[:3]):
            preview = result_item["response"][:80].replace('\n', ' ')
            print(f"   Q{i+1}: {preview}...")
        
        # Save detailed results to file
        self.save_detailed_results(results, result, total_test_time)
        
        # Resource usage
        print(f"\nüéÆ RESOURCE EFFICIENCY")
        if torch.cuda.is_available():
            gpu_memory_used = torch.cuda.memory_allocated() / (1024**3)
            gpu_memory_max = torch.cuda.max_memory_allocated() / (1024**3)
            print(f"   GPU Memory Used: {gpu_memory_used:.2f}GB")
            print(f"   GPU Memory Peak: {gpu_memory_max:.2f}GB")
        
        cpu_percent = psutil.cpu_percent()
        memory_percent = psutil.virtual_memory().percent
        print(f"   CPU Usage: {cpu_percent:.1f}%")
        print(f"   Memory Usage: {memory_percent:.1f}%")
    
    def save_detailed_results(self, results, full_result, total_test_time):
        """Save detailed results and responses to files"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save full responses
        responses_file = f"parallel_test_responses_{timestamp}.json"
        detailed_results = {
            "test_metadata": {
                "timestamp": timestamp,
                "num_queries": len(results),
                "total_test_time": total_test_time,
                "true_parallel": full_result.get("true_parallel", False),
                "parallel_efficiency": full_result.get("queries_per_second", 0) * total_test_time
            },
            "performance_metrics": {
                "total_time": total_test_time,
                "retrieval_time": full_result.get("retrieval_time", 0),
                "generation_time": full_result.get("generation_time", 0),
                "avg_tokens_per_second": full_result.get("avg_tokens_per_second", 0),
                "queries_per_second": full_result.get("queries_per_second", 0),
                "total_tokens": full_result.get("total_new_tokens", 0)
            },
            "query_results": []
        }
        
        for i, result in enumerate(results):
            detailed_results["query_results"].append({
                "query_id": i + 1,
                "question": result["question"],
                "response": result["response"],
                "response_length": result["response_length"],
                "chunks_found": result["chunks_found"]
            })
        
        with open(responses_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False)
        
        # Also save a readable text version
        readable_file = f"parallel_test_readable_{timestamp}.txt"
        with open(readable_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("TRUE PARALLEL LLM INFERENCE TEST RESULTS\n")
            f.write("=" * 80 + "\n")
            f.write(f"Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Total Test Time: {total_test_time:.2f} seconds\n")
            f.write(f"Queries Processed: {len(results)}\n")
            f.write(f"Parallel Efficiency: {full_result.get('queries_per_second', 0) * total_test_time:.1f}x speedup\n")
            f.write(f"Generation Speed: {full_result.get('avg_tokens_per_second', 0):.1f} tokens/sec\n")
            f.write("=" * 80 + "\n\n")
            
            for i, result in enumerate(results):
                f.write(f"QUERY {i + 1}:\n")
                f.write("-" * 40 + "\n")
                f.write(f"Question: {result['question']}\n\n")
                f.write(f"Response:\n{result['response']}\n\n")
                f.write(f"Response Length: {result['response_length']} characters\n")
                f.write(f"Chunks Found: {result['chunks_found']}\n")
                f.write("=" * 80 + "\n\n")
        
        print(f"\nüìÅ RESULTS SAVED:")
        print(f"   üìÑ Detailed JSON: {responses_file}")
        print(f"   üìÑ Readable Text: {readable_file}")
        print(f"   üíæ All responses preserved for analysis")

def main():
    """Main execution"""
    print("‚ö° TRUE PARALLEL LLM INFERENCE TESTER")
    print("=" * 60)
    print("üéØ Target: REAL parallel processing of multiple queries")
    print("üöÄ Method: Batch inference with simultaneous GPU processing")
    print("‚ö° Focus: True parallelism, not sequential execution")
    print("=" * 60)
    
    try:
        # Initialize tester
        tester = TrueParallelTester(num_queries=10)
        
        # Setup bot
        if not tester.setup_bot():
            print("‚ùå Failed to setup bot")
            return False
        
        # Verify bot
        if not tester.verify_bot():
            print("‚ùå Bot verification failed")
            return False
        
        # Launch test
        print(f"\n‚ö° Launching TRUE PARALLEL test in 3 seconds...")
        for i in range(3, 0, -1):
            print(f"‚è∞ {i}...")
            time.sleep(1)
        
        print("‚ö° LAUNCHING TRUE PARALLEL TEST!")
        
        # Run test
        result, total_test_time = tester.run_true_parallel_test()
        
        # Analyze results
        tester.analyze_results(result, total_test_time)
        
        print(f"\n‚ö° TRUE PARALLEL TESTING COMPLETED!")
        print("=" * 60)
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
        print(f"üîç Full error: {traceback.format_exc()}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)